"""
åŒ—ä¹å·å¸‚ã”ã¿åˆ†åˆ¥ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ RAG ã‚µãƒ¼ãƒ“ã‚¹
- Embeddings: BGE-M3 (bge-m3:latest)
- Vector DB: Chroma (./chroma_db ã«æ°¸ç¶šåŒ–)
- LLM: Ollama (Swallow v0.5 gguf)
"""

import os
import re
import json
import shutil
import time
import unicodedata
from datetime import datetime
from typing import List, Dict, Any, AsyncGenerator
import asyncio

# ChromaDBãƒ†ãƒ¬ãƒ¡ãƒˆãƒªã‚’ç„¡åŠ¹åŒ–
os.environ["ANONYMIZED_TELEMETRY"] = "False"

import pandas as pd
import ollama
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings

from .logger import setup_logger, get_logger

# BM25ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from rank_bm25 import BM25Okapi
import jieba
import re
from collections import defaultdict
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from typing import Tuple, Union

# ===== ç’°å¢ƒå¤‰æ•° / æ—¢å®šå€¤ =====
EMBED_MODEL = os.getenv("EMBED_MODEL", "bge-m3:latest")
LLM_MODEL   = os.getenv("LLM_MODEL", "hf.co/mmnga/Llama-3.1-Swallow-8B-Instruct-v0.5-gguf:latest")
CHROMA_DIR  = os.getenv("CHROMA_DIR", "./chroma_db")
DATA_DIR    = os.getenv("DATA_DIR", "./data")

# å¬å›å¼·åº¦ï¼ˆç’°å¢ƒå¤‰æ•°ã§å¯èª¿æ•´ï¼‰
DEFAULT_K   = int(os.getenv("RETRIEVER_K", "10"))
K_MAX       = int(os.getenv("RETRIEVER_K_MAX", "12"))
K_MIN       = int(os.getenv("RETRIEVER_K_MIN", "5"))

# ===== è»½é‡ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚° =====
_ZERO_WIDTH_TRANS = dict.fromkeys([0x200B, 0x200C, 0x200D, 0x2060, 0xFEFF], None)

# ===== è¡¨è¨˜æºã‚Œãƒ»åŒç¾©èªè¾æ›¸ =====
SYNONYMS_MAP = {
    # ã‚¢ãƒ«ãƒŸé–¢é€£
    "ã‚¢ãƒ«ãƒŸç¼¶": ["ã‚¢ãƒ«ãƒŸã‹ã‚“", "ã‚¢ãƒ«ãƒŸã‚«ãƒ³", "ã‚ã‚‹ã¿ã‹ã‚“", "ã‚ã‚‹ã¿ç¼¶"],
    "ã‚¢ãƒ«ãƒŸã‹ã‚“": ["ã‚¢ãƒ«ãƒŸç¼¶", "ã‚¢ãƒ«ãƒŸã‚«ãƒ³", "ã‚ã‚‹ã¿ã‹ã‚“", "ã‚ã‚‹ã¿ç¼¶"],
    
    # ã‚«ã‚¿ã‚«ãƒŠãƒ»ã²ã‚‰ãŒãªæºã‚Œ
    "ãƒšãƒƒãƒˆãƒœãƒˆãƒ«": ["ãƒšãƒƒãƒˆ", "ãºã£ã¨", "ãƒšãƒƒãƒˆãƒœãƒˆãƒ«"],
    "ãƒ—ãƒ©ã‚¹ãƒãƒƒã‚¯": ["ãƒ—ãƒ©", "ã·ã‚‰"],
    
    # ç•¥èªãƒ»çœç•¥å½¢
    "ãƒ†ãƒ¬ãƒ“": ["TV", "tv", "ãƒ†ã‚£ãƒ¼ãƒ–ã‚¤", "ã¦ã‚Œã³"],
    "ã‚¨ã‚¢ã‚³ãƒ³": ["ã‚¨ã‚¢ãƒ¼ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ¼", "ã‚¯ãƒ¼ãƒ©ãƒ¼", "ãˆã‚ã“ã‚“"],
    
    # ã‚«ãƒ³ãƒ»ã³ã‚“é–¢é€£
    "ç¼¶": ["ã‹ã‚“", "ã‚«ãƒ³"],
    "ç“¶": ["ã³ã‚“", "ãƒ“ãƒ³", "ã‚¬ãƒ©ã‚¹ç“¶"],
    
    # å®¶é›»è£½å“
    "å†·è”µåº«": ["ã‚Œã„ãã†ã“", "å†·å‡åº«"],
    "æ´—æ¿¯æ©Ÿ": ["ã›ã‚“ãŸãã", "æ´—æ¿¯æ©Ÿ"],
    
    # ä¸€èˆ¬çš„ãªè¡¨è¨˜æºã‚Œ
    "æºå¸¯é›»è©±": ["æºå¸¯", "ã‚¹ãƒãƒ›", "ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³", "ã‘ã„ãŸã„"],
    "ä¹¾é›»æ± ": ["é›»æ± ", "ã§ã‚“ã¡", "ãƒãƒƒãƒ†ãƒªãƒ¼"],
}

def expand_query_with_synonyms(query: str) -> List[str]:
    """
    ã‚¯ã‚¨ãƒªã‚’åŒç¾©èªã§æ‹¡å¼µ
    """
    queries = [query]
    query_lower = query.lower()
    
    # å®Œå…¨ä¸€è‡´ã§ã®åŒç¾©èªå±•é–‹
    for key, synonyms in SYNONYMS_MAP.items():
        if key in query:
            for synonym in synonyms:
                new_query = query.replace(key, synonym)
                if new_query not in queries:
                    queries.append(new_query)
        
        # åŒç¾©èªã‹ã‚‰ã‚­ãƒ¼ã¸ã®å±•é–‹
        for synonym in synonyms:
            if synonym in query:
                new_query = query.replace(synonym, key)
                if new_query not in queries:
                    queries.append(new_query)
    
    return queries

def normalize_query(query: str) -> str:
    """
    ã‚¯ã‚¨ãƒªã®æ­£è¦åŒ–ï¼ˆã‚«ã‚¿ã‚«ãƒŠãƒ»ã²ã‚‰ãŒãªçµ±ä¸€ç­‰ï¼‰
    """
    # å…¨è§’â†’åŠè§’
    query = unicodedata.normalize("NFKC", query)
    
    # ã‚«ã‚¿ã‚«ãƒŠã‚’ã²ã‚‰ãŒãªã«å¤‰æ›ï¼ˆã‚ˆã‚Šå¯›å®¹ãªæ¤œç´¢ã®ãŸã‚ï¼‰
    normalized = ""
    for char in query:
        if 'ã‚¡' <= char <= 'ãƒ¶':
            # ã‚«ã‚¿ã‚«ãƒŠã‚’ã²ã‚‰ãŒãªã«
            normalized += chr(ord(char) - ord('ã‚¡') + ord('ã'))
        else:
            normalized += char
    
    return normalized

def clean_text(s: str) -> str:
    if not s:
        return ""
    s = unicodedata.normalize("NFKC", s)
    s = s.translate(_ZERO_WIDTH_TRANS)
    return s.strip()

def strip_quotes_and_brackets(s: str) -> str:
    s = s.strip()
    s = re.sub(r'^[ã€Œã€â€œ"ï¼ˆ(]+', '', s)
    s = re.sub(r'[ã€ã€â€"ï¼‰)]+$', '', s)
    return s.strip()

def extract_item_like(query: str) -> str:
    q = clean_text(query)
    m = re.search(r'ã€Œ(.+?)ã€', q)
    if m:
        return strip_quotes_and_brackets(m.group(1))
    tmp = re.split(r'[ã¯ã‚’ã«ã§ãŒã¨ã‚‚ã¸]|[?ï¼Ÿã€‚!ï¼ã€]', q, maxsplit=1)[0]
    tmp = strip_quotes_and_brackets(tmp)
    return tmp[:32].strip() if tmp else q

# ===== Embeddings ãƒ©ãƒƒãƒ‘ï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼‰=====
class FlexibleEmbeddings:
    """
    LangChain ã®åŸ‹ã‚è¾¼ã¿IFäº’æ›:
    - embed_documents(texts: List[str]) -> List[List[float]]
    - embed_query(text: str) -> List[float]
    """
    def __init__(self, model: str):
        self.model = model
        self.logger = get_logger(__name__)
        
        # Ruriãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯HuggingFaceEmbeddingsã‚’è©¦è¡Œ
        if "ruri" in model.lower():
            try:
                from langchain_huggingface import HuggingFaceEmbeddings
                self.inner = HuggingFaceEmbeddings(
                    model_name=model,
                    model_kwargs={'device': 'cuda'},
                    encode_kwargs={'normalize_embeddings': True}
                )
                self.is_ruri = True
                self.logger.info(f"Ruriãƒ¢ãƒ‡ãƒ« {model} ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            except Exception as e:
                self.logger.warning(f"Ruriãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—ã€Ollamaã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: nomic-embed-textã‚’ä½¿ç”¨
                fallback_model = "nomic-embed-text"
                self.inner = OllamaEmbeddings(model=fallback_model)
                self.is_ruri = False
                self.logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ« {fallback_model} ã‚’ä½¿ç”¨ã—ã¾ã™")
        else:
            # å¾“æ¥ã®Ollamaãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
            self.inner = OllamaEmbeddings(model=model)
            self.is_ruri = False

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        if self.is_ruri:
            # Ruriãƒ¢ãƒ‡ãƒ«ã¯å‰ç½®è©ä¸è¦ã€ç›´æ¥ã‚¯ãƒªãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨
            texts2 = [clean_text(t) for t in texts]
        else:
            # BGEç³»ãƒ¢ãƒ‡ãƒ«ã«ã¯å‰ç½®è©ã‚’ä»˜ã‘ã‚‹ï¼ˆnomic-embed-textã¯ä¸è¦ï¼‰
            if "bge" in self.model.lower():
                texts2 = [f"passage: {clean_text(t)}" for t in texts]
            else:
                texts2 = [clean_text(t) for t in texts]
        return self.inner.embed_documents(texts2)

    def embed_query(self, text: str) -> List[float]:
        if self.is_ruri:
            # Ruriãƒ¢ãƒ‡ãƒ«ã¯å‰ç½®è©ä¸è¦
            return self.inner.embed_query(clean_text(text))
        else:
            # BGEç³»ãƒ¢ãƒ‡ãƒ«ã«ã¯å‰ç½®è©ã‚’ä»˜ã‘ã‚‹ï¼ˆnomic-embed-textã¯ä¸è¦ï¼‰
            if "bge" in self.model.lower():
                return self.inner.embed_query(f"query: {clean_text(text)}")
            else:
                return self.inner.embed_query(clean_text(text))

# ===== ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¯ãƒ©ã‚¹ =====
class HybridRetriever:
    """
    BM25ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰ã¨BGE-M3ï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ï¼‰ã‚’çµ„ã¿åˆã‚ã›ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
    """
    
    def __init__(self, chroma_retriever, documents: List[Document], alpha: float = 0.5):
        """
        Args:
            chroma_retriever: ChromaDBã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            documents: å…¨ã¦ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            alpha: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é‡ã¿ (0.0=BM25ã®ã¿, 1.0=ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿)
        """
        self.chroma_retriever = chroma_retriever
        self.documents = documents
        self.alpha = alpha
        self.logger = get_logger(__name__)  # ãƒ­ã‚¬ãƒ¼ã‚’è¿½åŠ 
        
        # BM25ç”¨ã®æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼è¨­å®š
        self.tokenizer = self._setup_tokenizer()
        
        # BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        self.bm25 = self._build_bm25_index()
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDãƒãƒƒãƒ”ãƒ³ã‚°
        self.doc_id_to_index = {doc.metadata.get('id', i): i for i, doc in enumerate(documents)}
        
        self.logger = setup_logger(__name__)
    
    def _setup_tokenizer(self):
        """æ—¥æœ¬èªç”¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨­å®š"""
        def japanese_tokenizer(text: str) -> List[str]:
            # jiebaã«ã‚ˆã‚‹æ—¥æœ¬èªåˆ†å‰²
            tokens = list(jieba.cut(text, cut_all=False))
            
            # è¿½åŠ ã®å‰å‡¦ç†
            processed_tokens = []
            for token in tokens:
                token = token.strip()
                if len(token) >= 1 and not re.match(r'^[ã€ã€‚ï¼ï¼Ÿ\s]+$', token):
                    processed_tokens.append(token.lower())
            
            return processed_tokens
        
        return japanese_tokenizer
    
    def _build_bm25_index(self) -> BM25Okapi:
        """BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰"""
        self.logger.info("BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ä¸­...")
        
        # å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokenized_docs = []
        for doc in self.documents:
            # ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            content = doc.page_content
            if doc.metadata:
                # é‡è¦ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¢ã‚¤ãƒ†ãƒ åã€åˆ†åˆ¥æ–¹æ³•ãªã©ï¼‰ã‚’å†…å®¹ã«è¿½åŠ 
                item_name = doc.metadata.get('item_name', '')
                category = doc.metadata.get('category', '')
                notes = doc.metadata.get('notes', '')
                
                combined_content = f"{content} {item_name} {category} {notes}"
            else:
                combined_content = content
                
            tokens = self.tokenizer(combined_content)
            tokenized_docs.append(tokens)
        
        bm25 = BM25Okapi(tokenized_docs)
        self.logger.info(f"BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†: {len(tokenized_docs)} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
        
        return bm25
    
    def _normalize_scores(self, scores: List[float]) -> List[float]:
        """ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ– (0-1)"""
        if not scores:
            return scores
            
        min_score = min(scores)
        max_score = max(scores)
        
        if max_score == min_score:
            return [1.0] * len(scores)
        
        return [(score - min_score) / (max_score - min_score) for score in scores]
    
    def hybrid_search(self, query: str, k: int = 10) -> List[Document]:
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®å®Ÿè¡Œ
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: è¿”ã™ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            
        Returns:
            ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        """
        self.logger.info(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å®Ÿè¡Œ: '{query}' (k={k})")
        
        # 1. BM25æ¤œç´¢
        bm25_scores = self._bm25_search(query, k * 2)  # ã‚ˆã‚Šå¤šãå–å¾—ã—ã¦å¤šæ§˜æ€§ç¢ºä¿
        
        # 2. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ (BGE-M3)
        vector_results = self._vector_search(query, k * 2)
        
        # 3. ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–ã¨çµåˆ
        hybrid_scores = self._combine_scores(bm25_scores, vector_results, k)
        
        # 4. ä¸Šä½kä»¶ã‚’è¿”ã™
        top_docs = [doc for doc, score in hybrid_scores[:k]]
        
        self.logger.info(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å®Œäº†: {len(top_docs)} ä»¶å–å¾—")
        return top_docs
    
    def _bm25_search(self, query: str, k: int) -> Dict[int, float]:
        """BM25æ¤œç´¢"""
        # ã‚¯ã‚¨ãƒªã®åŒç¾©èªæ‹¡å¼µ
        expanded_queries = expand_query_with_synonyms(query)
        
        # å„æ‹¡å¼µã‚¯ã‚¨ãƒªã§ã‚¹ã‚³ã‚¢è¨ˆç®—
        combined_scores = defaultdict(float)
        
        for expanded_query in expanded_queries:
            tokenized_query = self.tokenizer(expanded_query)
            if not tokenized_query:
                continue
                
            scores = self.bm25.get_scores(tokenized_query)
            
            # é‡ã¿ä»˜ãåŠ ç®—ï¼ˆå…ƒã‚¯ã‚¨ãƒªã«æœ€å¤§é‡ã¿ï¼‰
            weight = 1.0 if expanded_query == query else 0.7
            
            for doc_idx, score in enumerate(scores):
                combined_scores[doc_idx] += score * weight
        
        # ä¸Šä½kä»¶ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã‚¹ã‚³ã‚¢
        sorted_scores = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        return dict(sorted_scores[:k])
    
    def _vector_search(self, query: str, k: int) -> List[Tuple[Document, float]]:
        """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ (BGE-M3)"""
        try:
            # ChromaDBã®similarity_search_with_scoreã‚’ä½¿ç”¨
            results = self.chroma_retriever.similarity_search_with_score(query, k=k)
            return results
        except Exception as e:
            self.logger.error(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _combine_scores(self, bm25_scores: Dict[int, float], vector_results: List[Tuple[Document, float]], k: int) -> List[Tuple[Document, float]]:
        """BM25ã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ã‚³ã‚¢ã®çµåˆ"""
        
        # BM25ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–
        if bm25_scores:
            bm25_values = list(bm25_scores.values())
            normalized_bm25 = self._normalize_scores(bm25_values)
            bm25_normalized = dict(zip(bm25_scores.keys(), normalized_bm25))
        else:
            bm25_normalized = {}
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–ï¼ˆè·é›¢ã‚¹ã‚³ã‚¢ã‚’é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã«å¤‰æ›ï¼‰
        vector_scores = {}
        if vector_results:
            # ChromaDBã¯è·é›¢ã‚’è¿”ã™ã®ã§ã€é¡ä¼¼åº¦ã«å¤‰æ› (1 / (1 + distance))
            distances = [score for doc, score in vector_results]
            similarities = [1 / (1 + dist) if dist >= 0 else 1.0 for dist in distances]
            normalized_vector = self._normalize_scores(similarities)
            
            for (doc, _), norm_score in zip(vector_results, normalized_vector):
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã¾ãŸã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
                doc_id = doc.metadata.get('id')
                if doc_id is not None:
                    doc_idx = self.doc_id_to_index.get(doc_id)
                else:
                    # IDãŒãªã„å ´åˆã€å†…å®¹ã§ãƒãƒƒãƒãƒ³ã‚°ï¼ˆéåŠ¹ç‡ã ãŒä»•æ–¹ãªã—ï¼‰
                    doc_idx = None
                    for i, candidate_doc in enumerate(self.documents):
                        if candidate_doc.page_content == doc.page_content:
                            doc_idx = i
                            break
                
                if doc_idx is not None:
                    vector_scores[doc_idx] = norm_score
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢è¨ˆç®—
        hybrid_scores = {}
        all_doc_indices = set(bm25_normalized.keys()) | set(vector_scores.keys())
        
        for doc_idx in all_doc_indices:
            bm25_score = bm25_normalized.get(doc_idx, 0.0)
            vector_score = vector_scores.get(doc_idx, 0.0)
            
            # é‡ã¿ä»˜ãçµåˆ
            hybrid_score = (1 - self.alpha) * bm25_score + self.alpha * vector_score
            hybrid_scores[doc_idx] = hybrid_score
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ãƒšã‚¢ã‚’ä½œæˆ
        doc_score_pairs = []
        for doc_idx, score in hybrid_scores.items():
            if 0 <= doc_idx < len(self.documents):
                doc_score_pairs.append((self.documents[doc_idx], score))
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)
        
        return doc_score_pairs[:k]
    
    def get_retrieval_stats(self) -> Dict[str, Any]:
        """æ¤œç´¢çµ±è¨ˆæƒ…å ±"""
        return {
            "total_documents": len(self.documents),
            "bm25_ready": self.bm25 is not None,
            "vector_ready": self.chroma_retriever is not None,
            "alpha": self.alpha,
            "tokenizer": "jieba"
        }

# ===== Index manifest ï¼ˆåŸ‹ã‚è¾¼ã¿ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ï¼‰=====
MANIFEST_FILE = "manifest.json"
MANIFEST_STRATEGY = "bge_query_passage_prefix_v1"

def _manifest_path() -> str:
    return os.path.join(CHROMA_DIR, MANIFEST_FILE)

def _load_manifest() -> Dict[str, Any] | None:
    try:
        with open(_manifest_path(), "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def _write_manifest() -> None:
    os.makedirs(CHROMA_DIR, exist_ok=True)
    with open(_manifest_path(), "w", encoding="utf-8") as f:
        json.dump(
            {
                "embed_model": EMBED_MODEL,
                "strategy": MANIFEST_STRATEGY,
                "created_at": datetime.now().isoformat(),
            },
            f,
            ensure_ascii=False,
            indent=2,
        )

def _manifest_mismatch() -> bool:
    m = _load_manifest()
    if not m:
        return True
    return not (
        m.get("embed_model") == EMBED_MODEL
        and m.get("strategy") == MANIFEST_STRATEGY
    )

# ===== æœ¬ä½“ =====
class KitakyushuWasteRAGService:
    """RAGã®åˆæœŸåŒ–ãƒ»CSVå–ã‚Šè¾¼ã¿ãƒ»æ¤œç´¢ãƒ»å¿œç­”ç”Ÿæˆ"""

    def __init__(self):
        self.logger = setup_logger(__name__)

        self.embeddings = FlexibleEmbeddings(model=EMBED_MODEL)

        if os.path.isdir(CHROMA_DIR) and _manifest_mismatch():
            self.logger.warning("Embedding è¨­å®šãŒæ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ä¸ä¸€è‡´ã®ãŸã‚ã€å†æ§‹ç¯‰ã—ã¾ã™ã€‚")
            shutil.rmtree(CHROMA_DIR, ignore_errors=True)

        os.makedirs(CHROMA_DIR, exist_ok=True)
        self.vectorstore = Chroma(
            persist_directory=CHROMA_DIR,
            embedding_function=self.embeddings
        )

        os.makedirs(DATA_DIR, exist_ok=True)
        self._load_csv_dir()
        _write_manifest()

        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®åˆæœŸåŒ–
        self.hybrid_retriever = None
        self._initialize_hybrid_search()

        self.logger.info(
            f"RAG ready | EMBED_MODEL={EMBED_MODEL} | LLM_MODEL={LLM_MODEL} | "
            f"CHROMA_DIR={CHROMA_DIR} | DATA_DIR={DATA_DIR} | Hybrid Search: {'Enabled' if self.hybrid_retriever else 'Disabled'}"
        )

    # ========= CSV èª­ã¿è¾¼ã¿ =========
    def _row_to_text(self, row: pd.Series) -> str:
        item  = row.get("å“å") or row.get("å“ç›®") or row.get("item") or ""
        how   = row.get("å‡ºã—æ–¹") or row.get("å‡¦ç†æ–¹æ³•") or row.get("how") or ""
        note  = row.get("å‚™è€ƒ") or row.get("æ³¨æ„") or row.get("note") or ""
        area  = row.get("ã‚¨ãƒªã‚¢") or row.get("åœ°åŒº") or row.get("area") or ""
        return "\n".join([
            f"å“ç›®: {str(item).strip()}",
            f"å‡ºã—æ–¹: {str(how).strip()}",
            f"å‚™è€ƒ: {str(note).strip()}",
            f"ã‚¨ãƒªã‚¢: {str(area).strip()}",
        ]).strip()

    def add_csv(self, filepath: str) -> Dict[str, Any]:
        if not os.path.exists(filepath):
            return {"success": False, "error": f"CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}"}
            
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª
        filename = os.path.basename(filepath)
        existing_sources = self.get_data_sources()
        
        if existing_sources.get("success"):
            for source in existing_sources.get("sources", []):
                if source.get("file_name") == filename:
                    self.logger.warning(f"CSVé‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {filename} ã¯æ—¢ã«ç™»éŒ²æ¸ˆã¿")
                    return {"success": True, "count": 0, "message": f"ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã¯æ—¢ã«ç™»éŒ²æ¸ˆã¿ã§ã™"}
        
        try:
            df = pd.read_csv(filepath, encoding="utf-8")
        except UnicodeDecodeError:
            df = pd.read_csv(filepath, encoding="cp932")

        docs: List[Document] = []
        for _, row in df.iterrows():
            text = self._row_to_text(row)
            if text:
                docs.append(Document(page_content=text, metadata={"source": filename}))
        if docs:
            self.vectorstore.add_documents(docs)
            self.logger.info(f"CSVå–ã‚Šè¾¼ã¿: {filepath} | æ–‡æ›¸æ•°={len(docs)}")
            return {"success": True, "count": len(docs)}
        return {"success": True, "count": 0}

    def _load_csv_dir(self) -> None:
        total = 0
        for fn in os.listdir(DATA_DIR):
            if fn.lower().endswith(".csv"):
                res = self.add_csv(os.path.join(DATA_DIR, fn))
                total += int(res.get("count", 0))
        self.logger.info(f"åˆæœŸCSVå–ã‚Šè¾¼ã¿å®Œäº†: {total} æ–‡æ›¸")

    # ========= æ¤œç´¢ï¼ˆå¼·åŒ–ç‰ˆï¼‰ =========
    def _format_docs(self, docs: List[Document], limit_each: int = 320, max_docs: int = 8) -> str:
        if not docs:
            return "é–¢é€£æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        chunks = []
        for i, d in enumerate(docs[:max_docs], start=1):
            txt = (d.page_content or "").strip()
            if len(txt) > limit_each:
                txt = txt[:limit_each] + "â€¦"
            chunks.append(f"[å€™è£œ{i}]\n{txt}")
        return "\n\n".join(chunks)

    def _initialize_hybrid_search(self):
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®åˆæœŸåŒ–"""
        try:
            # å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
            all_docs = []
            collection = self.vectorstore._collection
            if collection is not None:
                # ChromaDBã‹ã‚‰å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
                result = collection.get()
                if result and 'documents' in result:
                    for i, content in enumerate(result['documents']):
                        metadata = result.get('metadatas', [{}])[i] if i < len(result.get('metadatas', [])) else {}
                        doc = Document(page_content=content, metadata=metadata)
                        all_docs.append(doc)
            
            if all_docs:
                # ChromaDBã®retrieverã‚’ä½œæˆ
                chroma_retriever = self.vectorstore.as_retriever(search_kwargs={"k": DEFAULT_K * 2})
                
                # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’åˆæœŸåŒ–
                self.hybrid_retriever = HybridRetriever(
                    chroma_retriever=chroma_retriever,
                    documents=all_docs,
                    alpha=0.6  # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’60%ã€BM25ã‚’40%ã®é‡ã¿ã§çµåˆ
                )
                self.logger.info(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢åˆæœŸåŒ–å®Œäº†: {len(all_docs)} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
            else:
                self.logger.warning("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’ç„¡åŠ¹åŒ–")
                self.hybrid_retriever = None
                
        except Exception as e:
            self.logger.error(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            self.hybrid_retriever = None

    def hybrid_similarity_search(self, query: str, k: int = DEFAULT_K, use_hybrid: bool = True) -> List[Document]:
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆBM25 + BGE-M3ï¼‰ã¾ãŸã¯ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿ã‚’å®Ÿè¡Œ
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: è¿”ã™ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            use_hybrid: Trueãªã‚‰ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã€Falseãªã‚‰ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿
        """
        k = max(K_MIN, min(k or DEFAULT_K, K_MAX))
        
        self.logger.info(f"æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: query='{query}', k={k}, use_hybrid={use_hybrid}, hybrid_retriever={self.hybrid_retriever is not None}")
        
        if use_hybrid and self.hybrid_retriever:
            self.logger.info(f"ğŸ”¥ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å®Ÿè¡Œ: '{query}' (k={k})")
            try:
                results = self.hybrid_retriever.hybrid_search(query, k)
                self.logger.info(f"âœ… ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢æˆåŠŸ: {len(results)} ä»¶å–å¾—")
                return results
            except Exception as e:
                self.logger.error(f"âŒ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
                return self.similarity_search(query, k)
        else:
            self.logger.info(f"ğŸ“Š ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ: '{query}' (k={k})")
            return self.similarity_search(query, k)

    def similarity_search(self, query: str, k: int = DEFAULT_K) -> List[Document]:
        k = max(K_MIN, min(k or DEFAULT_K, K_MAX))
        q_clean = clean_text(query)
        
        # åŒç¾©èªæ‹¡å¼µã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
        expanded_queries = expand_query_with_synonyms(q_clean)
        self.logger.info(f"Expanded queries: {expanded_queries}")
        
        item_q = extract_item_like(q_clean)

        def item_match_score(txt: str, item: str) -> int:
            if not txt:
                return 0
            m = re.search(r"å“ç›®:\s*(.+)", txt)
            name = (m.group(1) if m else "").strip()
            if not name:
                return 0
            n1 = clean_text(name)
            n2 = clean_text(item)
            
            # åŒç¾©èªã‚‚è€ƒæ…®ã—ãŸãƒãƒƒãƒãƒ³ã‚°
            if n1 == n2:
                return 3
            
            # åŒç¾©èªãƒã‚§ãƒƒã‚¯
            for key, synonyms in SYNONYMS_MAP.items():
                if (n1 == key and n2 in synonyms) or (n2 == key and n1 in synonyms):
                    return 2
                if n1 in synonyms and n2 in synonyms:
                    return 2
            
            return 1 if (n2 and (n2 in n1 or n1 in n2)) else 0

        def rerank_by_item(docs, item):
            return sorted(docs, key=lambda d: item_match_score((d.page_content or ""), item), reverse=True)

        def poor(dlist: List[Document], item_hint: str) -> bool:
            if not dlist:
                return True
            heads = sum(1 for d in dlist if "å“ç›®:" in (d.page_content or ""))
            has_item = any(item_match_score((d.page_content or ""), item_hint) > 0 for d in dlist)
            return heads < max(1, len(dlist)//3) or not has_item

        def merge_dedup(lists):
            seen = set()
            out = []
            for lst in lists:
                for d in lst or []:
                    key = ((d.page_content or "").strip(), json.dumps(getattr(d, "metadata", {}), ensure_ascii=False, sort_keys=True))
                    if key in seen:
                        continue
                    seen.add(key)
                    out.append(d)
            return out

        all_docs = []
        
        # å„æ‹¡å¼µã‚¯ã‚¨ãƒªã§æ¤œç´¢ã‚’å®Ÿè¡Œ
        for expanded_query in expanded_queries:
            try:
                docs_main = self.vectorstore.similarity_search(expanded_query, k=k)
                all_docs.extend(docs_main)
                self.logger.info(f"Found {len(docs_main)} docs for query: {expanded_query}")
            except Exception as e:
                self.logger.warning(f"similarity_search failed for '{expanded_query}': {e}")

        # é‡è¤‡ã‚’é™¤å»
        all_docs = merge_dedup([all_docs])
        all_docs = rerank_by_item(all_docs, item_q)
        
        # ååˆ†ãªçµæœãŒå¾—ã‚‰ã‚ŒãŸå ´åˆã¯ã“ã“ã§çµ‚äº†
        if not poor(all_docs, item_q) and len(all_docs) >= k//2:
            return all_docs[:k]

        # è¿½åŠ æ¤œç´¢ï¼ˆã‚¢ã‚¤ãƒ†ãƒ åã§ã®æ¤œç´¢ï¼‰
        if item_q:
            # ã‚¢ã‚¤ãƒ†ãƒ åã‚‚åŒç¾©èªæ‹¡å¼µ
            expanded_items = expand_query_with_synonyms(item_q)
            for expanded_item in expanded_items:
                try:
                    docs_item = self.vectorstore.similarity_search(expanded_item, k=k)
                    all_docs.extend(docs_item)
                except Exception as e:
                    self.logger.warning(f"similarity_search failed for item '{expanded_item}': {e}")

                # æœ€çµ‚çš„ãªé‡è¤‡é™¤å»ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        final_docs = merge_dedup([all_docs])
        final_docs = rerank_by_item(final_docs, item_q)
        
        return final_docs[:k]

    def format_documents(self, docs: List[Document], limit_each: int = 150) -> str:

        # 3) å“ç›®æŠ½å‡ºã‚¯ã‚¨ãƒªã§å†æ¤œç´¢ï¼ˆé€šå¸¸â†’MMRï¼‰
        try:
            docs2 = self.vectorstore.similarity_search(item_q, k=k)
        except Exception as e:
            self.logger.warning(f"similarity_search failed (item): {e}")
            docs2 = []
        docs2 = rerank_by_item(docs2, item_q)
        if not poor(docs2, item_q):
            return docs2[:k]

        try:
            docs2_mmr = self.vectorstore.max_marginal_relevance_search(item_q, k=k, fetch_k=max(k*2, 8))
        except Exception as e:
            self.logger.warning(f"MMR search failed (item): {e}")
            docs2_mmr = []
        docs2_mmr = rerank_by_item(docs2_mmr, item_q)
        return docs2_mmr[:k]

    # ========= LLM å‘¼ã³å‡ºã— =========
    def _call_llm(self, prompt: str) -> str:
        res = ollama.chat(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            options={"temperature": 0.1, "num_ctx": 4096},
        )
        return (res.get("message", {}) or {}).get("content", "")

    # ========= ãƒ¦ãƒ¼ã‚¶ãƒ¼API =========
    def blocking_query(self, query: str, k: int = DEFAULT_K, use_hybrid: bool = True) -> Dict[str, Any]:
        """
        ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°æ¤œç´¢ã‚¯ã‚¨ãƒª
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: æ¤œç´¢ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            use_hybrid: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
        """
        t0 = time.time()
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã¾ãŸã¯å¾“æ¥ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’é¸æŠ
        if use_hybrid and self.hybrid_retriever:
            docs = self.hybrid_similarity_search(query, k=k, use_hybrid=True)
            search_method = "hybrid"
        else:
            docs = self.similarity_search(query, k=k)  
            search_method = "vector_only"
            
        ctx = self._format_docs(docs)

        prompt = (
            "ã‚ãªãŸã¯åŒ—ä¹å·å¸‚ã®ã”ã¿åˆ†åˆ¥æ¡ˆå†…ã®å°‚é–€AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
            "åŒ—ä¹å·å¸‚æ°‘ã®çš†æ§˜ã«æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„ã”ã¿åˆ†åˆ¥æƒ…å ±ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒä½¿å‘½ã§ã™ã€‚"
            "ä»¥ä¸‹ã®å‚ç…§ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å†…ã§ã€æ—¥æœ¬èªã§ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"
            "\nå›ç­”å½¢å¼:"
            "ã€Œå‡ºã—æ–¹: [å…·ä½“çš„ãªåˆ†åˆ¥æ–¹æ³•]ã€ã‚’æœ€åˆã«æ˜è¨˜ã—ã€å¿…è¦ã«å¿œã˜ã¦ã€Œå‚™è€ƒ: [è¿½åŠ æƒ…å ±]ã€ã‚’è£œè¶³ã—ã¦ãã ã•ã„ã€‚"
            "\né‡è¦ãªãƒ«ãƒ¼ãƒ«:"
            "1. è³ªå•ã•ã‚ŒãŸå“ç›®ã«é–¢é€£ã™ã‚‹æƒ…å ±ã®ã¿ã‚’å›ç­”ã—ã¦ãã ã•ã„ï¼ˆç„¡é–¢ä¿‚ãªå“ç›®æƒ…å ±ã¯å«ã‚ãªã„ï¼‰"
            "2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è©²å½“æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€è©²å½“ã™ã‚‹æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚åŒ—ä¹å·å¸‚ã®ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ï¼ˆhttps://www.city.kitakyushu.lg.jp/ï¼‰ã§ã”ç¢ºèªã„ãŸã ãã‹ã€ãŠä½ã¾ã„ã®åŒºå½¹æ‰€ç’°å¢ƒèª²ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚ã€ã¨å›ç­”ã—ã¦ãã ã•ã„"
            "3. å›ç­”ã¯ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ãã€å¿…ãšã€Œå‡ºã—æ–¹ã€ã‚’å«ã‚ã¦ãã ã•ã„"
            "4. æ¨æ¸¬ã‚„ä¸€èˆ¬çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã¯å³ç¦ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŸºã¥ã„ãŸæ­£ç¢ºãªæƒ…å ±ã®ã¿ã‚’æä¾›ã—ã¦ãã ã•ã„"
            "5. è¤‡æ•°ã®é–¢é€£å“ç›®ãŒã‚ã‚‹å ´åˆã¯ã€è³ªå•ã«æœ€ã‚‚é©åˆã™ã‚‹ã‚‚ã®ã®ã¿ã‚’é¸ã‚“ã§å›ç­”ã—ã¦ãã ã•ã„"
            "6. ã‚¨ãƒªã‚¢æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ã€è©²å½“ã™ã‚‹åœ°åŒºåã‚‚ä½µè¨˜ã—ã¦ãã ã•ã„"
            "\n\nè³ªå•:\n"
            f"{clean_text(query)}\n\nå‚ç…§ãƒ‡ãƒ¼ã‚¿:\n{ctx}\n\nå›ç­”:"
        )
        try:
            answer = self._call_llm(prompt).strip()
            return {
                "response": answer,
                "documents": len(docs),
                "search_method": search_method,
                "hybrid_enabled": self.hybrid_retriever is not None,
                "latency": time.time() - t0,
                "timestamp": datetime.now().isoformat(),
            }
        except Exception as e:
            return {
                "response": f"ã‚¨ãƒ©ãƒ¼: {e}", 
                "documents": len(docs), 
                "search_method": search_method,
                "latency": time.time() - t0
            }

    async def streaming_query(self, query: str, k: int = DEFAULT_K, use_hybrid: bool = True) -> AsyncGenerator[str, None]:
        """
        ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¤œç´¢ã‚¯ã‚¨ãƒª
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: æ¤œç´¢ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°  
            use_hybrid: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
        """
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã¾ãŸã¯å¾“æ¥ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’é¸æŠ
        if use_hybrid and self.hybrid_retriever:
            docs = self.hybrid_similarity_search(query, k=k, use_hybrid=True)
        else:
            docs = self.similarity_search(query, k=k)
            
        ctx = self._format_docs(docs)

        prompt = (
            "ã‚ãªãŸã¯åŒ—ä¹å·å¸‚ã®å„ªç§€ãªã”ã¿åˆ†åˆ¥æ¡ˆå†…ã®å°‚é–€AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
            "åŒ—ä¹å·å¸‚æ°‘ã®çš†æ§˜ã«æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„ã”ã¿åˆ†åˆ¥æƒ…å ±ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒä½¿å‘½ã§ã™ã€‚"
            "ä»¥ä¸‹ã®å‚ç…§ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å†…ã§ã€æ—¥æœ¬èªã§ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"
            "\nå›ç­”å½¢å¼:"
            "ã€Œå‡ºã—æ–¹: [å…·ä½“çš„ãªåˆ†åˆ¥æ–¹æ³•]ã€ã‚’æœ€åˆã«æ˜è¨˜ã—ã€å¿…è¦ã«å¿œã˜ã¦ã€Œå‚™è€ƒ: [è¿½åŠ æƒ…å ±]ã€ã‚’è£œè¶³ã—ã¦ãã ã•ã„ã€‚"
            "\né‡è¦ãªãƒ«ãƒ¼ãƒ«:"
            "1. è³ªå•ã•ã‚ŒãŸå“ç›®ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’å›ç­”ã—ã¦ãã ã•ã„ï¼ˆç„¡é–¢ä¿‚ãªå“ç›®æƒ…å ±ã¯å«ã‚ãªã„ï¼‰"
            "2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è©²å½“æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€è©²å½“ã™ã‚‹æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚åŒ—ä¹å·å¸‚ã®ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ï¼ˆhttps://www.city.kitakyushu.lg.jp/ï¼‰ã§ã”ç¢ºèªã„ãŸã ãã‹ã€ãŠä½ã¾ã„ã®åŒºå½¹æ‰€ç’°å¢ƒèª²ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚ã€ã¨å›ç­”ã—ã¦ãã ã•ã„"
            "3. å›ç­”ã¯ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ãã€å¿…ãšã€Œå‡ºã—æ–¹ã€ã‚’å«ã‚ã¦ãã ã•ã„"
            "4. æ¨æ¸¬ã‚„ä¸€èˆ¬çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã¯å³ç¦ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŸºã¥ã„ãŸæ­£ç¢ºãªæƒ…å ±ã®ã¿ã‚’æä¾›ã—ã¦ãã ã•ã„"
            "5. è¤‡æ•°ã®é–¢é€£å“ç›®ãŒã‚ã‚‹å ´åˆã¯ã€è³ªå•ã«æœ€ã‚‚é©åˆã™ã‚‹ã‚‚ã®ã‚’é¸ã‚“ã§å›ç­”ã—ã¦ãã ã•ã„ã€å¿…è¦ã«å¿œã˜ã¦é¡ä¼¼ã®å“ç›®ã‚‚è£œè¶³ã—ã¦ãã ã•ã„"
            "6. ã‚¨ãƒªã‚¢æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ã€è©²å½“ã™ã‚‹åœ°åŒºåã‚‚ä½µè¨˜ã—ã¦ãã ã•ã„"
            f"\n\nè³ªå•:\n{clean_text(query)}\n\nå‚ç…§ãƒ‡ãƒ¼ã‚¿:\n{ctx}\n\nå›ç­”:"
        )

        try:
        # æ³¨æ„ï¼šollama.chat æ˜¯åŒæ­¥ç”Ÿæˆå™¨ï¼›åœ¨ async å‡½æ•°å†…è¿­ä»£å®ƒä¼šé˜»å¡äº‹ä»¶å¾ªç¯ï¼Œ
        # ä½†æ¯æ¬¡ yield éƒ½ä¼šæŠŠå·²ç”Ÿæˆå†…å®¹åˆ·ç»™å®¢æˆ·ç«¯ï¼ˆSSE/StreamingResponse å¯æ­£å¸¸å·¥ä½œï¼‰ã€‚
        # å¦‚æœä½ å¸Œæœ›å®Œå…¨ä¸é˜»å¡äº‹ä»¶å¾ªç¯ï¼Œå¯å†å‡çº§ä¸ºçº¿ç¨‹ç”Ÿäº§è€… + asyncio.Queue æ¡¥æ¥ï¼ˆæˆ‘ä¹Ÿå¯ä»¥ç»™ä½ é‚£ç‰ˆï¼‰ã€‚
            stream = ollama.chat(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                stream=True,
                options={"temperature": 0.1, "num_ctx": 4096},
            )
            for chunk in stream:
                # æ­£ç¡®å¤„ç†æµç»“æŸä¿¡å·
                if chunk.get("done"):
                    break
                msg = chunk.get("message") or {}
                content = msg.get("content", "")
                if content:
                    # åªæŠŠéç©ºæ–‡æœ¬ç‰‡æ®µå‘ç»™å‰ç«¯
                    yield content
        except Exception as e:
            yield f"ã‚¨ãƒ©ãƒ¼: {e}"

    def get_hybrid_search_stats(self) -> Dict[str, Any]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        if not self.hybrid_retriever:
            return {
                "enabled": False,
                "reason": "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
            }
        
        hybrid_stats = self.hybrid_retriever.get_retrieval_stats()
        return {
            "enabled": True,
            "stats": hybrid_stats,
            "alpha": hybrid_stats.get("alpha", 0.5),
            "description": {
                "alpha": "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é‡ã¿ (0.0=BM25ã®ã¿, 1.0=ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿)",
                "bm25": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹æ¤œç´¢ (TF-IDFé¢¨ã®èªå½™ãƒãƒƒãƒãƒ³ã‚°)",
                "vector": "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ (BGE-M3åŸ‹ã‚è¾¼ã¿)",
                "tokenizer": "æ—¥æœ¬èªåˆ†å‰²å™¨ (jieba)"
            }
        }

    def get_data_sources(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ä¸€è¦§ã¨çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        try:
            # ChromaDBã‹ã‚‰å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            collection = self.vectorstore._collection
            all_data = collection.get(include=['metadatas', 'documents'])
            
            # ã‚½ãƒ¼ã‚¹åˆ¥ã®çµ±è¨ˆã‚’ä½œæˆ
            source_stats = {}
            total_documents = len(all_data['documents'])
            
            for i, metadata in enumerate(all_data['metadatas']):
                source = metadata.get('source', 'unknown')
                if source not in source_stats:
                    source_stats[source] = {
                        'file_name': source,
                        'document_count': 0,
                        'sample_content': [],
                        'last_updated': metadata.get('last_updated', 'N/A')
                    }
                
                source_stats[source]['document_count'] += 1
                
                # ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä¿å­˜ï¼ˆæœ€åˆã®3ã¤ã¾ã§ï¼‰
                if len(source_stats[source]['sample_content']) < 3:
                    doc_content = all_data['documents'][i]
                    if doc_content:
                        # å“ç›®åã‚’æŠ½å‡º
                        lines = doc_content.split('\n')
                        item_line = next((line for line in lines if line.startswith('å“ç›®:')), '')
                        if item_line:
                            item_name = item_line.replace('å“ç›®:', '').strip()
                            source_stats[source]['sample_content'].append(item_name)
            
            return {
                'success': True,
                'total_documents': total_documents,
                'total_sources': len(source_stats),
                'sources': list(source_stats.values()),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e),
                'total_documents': 0,
                'total_sources': 0,
                'sources': []
            }

    def get_sample_documents(self, source: str = None, limit: int = 10) -> Dict[str, Any]:
        """æŒ‡å®šã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—"""
        try:
            collection = self.vectorstore._collection
            
            if source:
                # ç‰¹å®šã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
                all_data = collection.get(
                    where={"source": source},
                    include=['metadatas', 'documents'],
                    limit=limit
                )
            else:
                # å…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—
                all_data = collection.get(
                    include=['metadatas', 'documents'],
                    limit=limit
                )
            
            documents = []
            for i, doc_content in enumerate(all_data['documents']):
                metadata = all_data['metadatas'][i]
                
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
                lines = doc_content.split('\n')
                parsed_doc = {}
                for line in lines:
                    if ':' in line:
                        key, value = line.split(':', 1)
                        parsed_doc[key.strip()] = value.strip()
                
                documents.append({
                    'source': metadata.get('source', 'unknown'),
                    'content': parsed_doc,
                    'raw_content': doc_content[:200] + '...' if len(doc_content) > 200 else doc_content
                })
            
            return {
                'success': True,
                'documents': documents,
                'count': len(documents),
                'source_filter': source
            }
            
        except Exception as e:
            self.logger.error(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e),
                'documents': [],
                'count': 0
            }

# ======= ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ =======
_rag = None
def get_rag_service() -> KitakyushuWasteRAGService:
    global _rag
    if _rag is None:
        _rag = KitakyushuWasteRAGService()
    return _rag
